<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="DiffTalker.">
  <meta name="keywords" content="Diffusion models, Talking faces, Transformers">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DiffTalker</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title text-muted">
            <span class="text-primary">ß·</span>

            <!-- DIFFTALKER: CO-DRIVEN AUDIO-IMAGE DIFFUSION FOR TALKING FACES VIA
INTERMEDIATE LANDMARKS -->
            <span class="text-dark">DIFFTALKER:</span>CO-DRIVEN AUDIO-IMAGE DIFFUSION FOR TALKING FACING via INTERMEDIATE LANDMARKS
          </h1>
          <p class="is-size-5 text-center">
	            Submittd to "ICASSP 2024"
		      </p>

          <!-- <p class="is-size-5 text-center">
            arXiv
          </p> -->

           <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/qizipeng">Zipeng Qi</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://largeaudiomodel.com/">Xulong Zhang</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://largeaudiomodel.com/">Ning Cheng</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://largeaudiomodel.com/">Jing Xiao</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://largeaudiomodel.com/">Jianzong Wang</a><sup>2</sup>,
            </span>
          </div>





<!--          <div class="is-size-5 publication-authors">-->
<!--            <span class="author-block">-->
<!--              <a href="https://scholar.google.com/citations?user=z8GwuTgAAAAJ&hl=en">Ashkan Mirzaei</a><sup>1,2</sup>,</span>-->
<!--            <span class="author-block">-->
<!--              <a href="https://scholar.google.ca/citations?user=mV9zdJMAAAAJ&hl=en">Tristan Aumentado-Armstrong</a><sup>1,2,4</sup>,</span>-->
<!--            <span class="author-block">-->
<!--              <a href="https://scholar.google.com/citations?user=3Br8x_gAAAAJ&hl=en&oi=ao">Konstantinos G. Derpanis</a><sup>1,3,4</sup>,-->
<!--            </span>-->
<!--            <span class="author-block">-->
<!--              <a href="https://scholar.google.com/citations?user=KtSR8_0AAAAJ&hl=en">Jonathan Kelly</a><sup>2</sup>,-->
<!--            </span>-->
<!--            <span class="author-block">-->
<!--              <a href="https://scholar.google.ca/citations?user=x2wyjkAAAAAJ&hl=en">Marcus A. Brubaker</a><sup>1,3,4</sup>,-->
<!--            </span>-->
<!--            <span class="author-block">-->
<!--              <a href="https://scholar.google.com/citations?user=Nuw1Y4oAAAAJ&hl=en">Igor Gilitschenski</a><sup>2</sup>,-->
<!--            </span>-->
<!--            <span class="author-block">-->
<!--              <a href="https://scholar.google.com/citations?user=7EFMKWUAAAAJ&hl=en">Alex Levinshtein</a><sup>1</sup>-->
<!--            </span>-->
<!--          </div>-->

          <div class="is-size-6 publication-authors">
            <span class="author-block text-muted">{qizipeng}@buaa.edu.cn,</span>
<!--            <span class="author-block text-muted">tristan.a@partner.samsung.com,</span>-->
<!--            <span class="author-block text-muted">{kosta,mab}@eecs.yorku.ca,</span>-->
<!--            <span class="author-block text-muted">alex.lev@samsung.com</span>-->
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Beihang University,</span>
            <span class="author-block"><sup>2</sup>Ping An Technology (Shenzhen) Co., Ltd.,</span>
<!--            <span class="author-block"><sup>2</sup>University of Toronto,</span>-->
<!--            <span class="author-block"><sup>3</sup>York University,</span>-->
<!--            <span class="author-block"><sup>4</sup>Vector Institute for AI</span>-->
            <!-- </br>
            <span class="author-block"><sup>*</sup>Denotes equal contribution</span> -->
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2309.07509.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
<!--               <span class="link-block">-->
<!--                <a href="paper.pdf"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fas fa-file-pdf"></i>-->
<!--                  </span>-->
<!--                  <span>Paper (High-Quality)</span>-->
<!--                </a>-->
<!--              </span>-->
<!--              <span class="link-block">-->
<!--                <a href="https://youtu.be/WEgJf1WC5SQ"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-youtube"></i>-->
<!--                  </span>-->
<!--                  <span>Video</span>-->
<!--                </a>-->
<!--              </span>-->
              <span class="link-block">
                <a href="code link"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (coming soon)</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="dataset link" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data (coming soon)</span>
                  </a>
              </span>
            </div>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section">
  <div class="container is-max-desktop"> -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Motivation</h2>
        <div class="content">
          <img src="my_data/motivation.PNG" width="70%"/>
        </div>
        </br>
        <div class="content has-text-justified">
          <p>
           The mainstream segmentation methods benefit from the deep convolution neural networks (CNN), which can effectively learn and extract robust and discriminative features from the input images. However, deep CNN-based remote sensing image segmentation methods rely heavily on massive training data. As shown in Fig.(a), the performance of traditional CNN-based methods is sensitive to the number of annotations. A large number of high-quality pixel-wise annotations, as a guarantee for the performance of CNN-based segmentation methods, consume a great deal of time and effort. As for the task of semantic segmentation for a 3D scene given only limited annotated views, the CNN-based methods may overfit the views in the training data but generate poor results for the rest of the views. The key reason is that the 2D texture information or 2D context relationship is insufficient to identify similar-textured objects (Fig. (b)) in a 3D scene. Finally, the 3D context relationship of a scene is also crucial for semantic attribute prediction (Fig. (c)). For example, the building is typically higher than the road and the same object across different views usually has a similar texture. However, these properties have been rarely investigated in previous papers.
          </p>
          </br>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- <img src="my_data/abstract.PNG" style="float: right; margin-left:20px; margin-bottom: 10px" width="40%;" /> -->
          <p class="content has-text-justified">
           Generating realistic talking faces is a complex and widely
            discussed task with numerous applications. In this paper,
            we present DiffTalker, a novel model designed to generate
            lifelike talking faces through audio and landmark co-driving.
            DiffTalker addresses the challenges associated with directly
            applying diffusion models to audio control, which are tradi-
            tionally trained on text-image pairs. DiffTalker consists of
            two agent networks: a transformer-based landmarks comple-
            tion network for geometric accuracy and a diffusion-based
            face generation network for texture details. Landmarks play
            a pivotal role in establishing a seamless connection between
            the audio and image domains, facilitating the incorporation of
            knowledge from pre-trained diffusion models. This innova-
            tive approach efficiently produces articulate-speaking faces.
            Experimental results showcase DiffTalkerâs superior perfor-
            mance in producing clear and geometrically accurate talking
            faces, all without the need for additional alignment between
            audio and image features.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--        <h2 class="title is-3">Video</h2>-->
<!--        <div class="publication-video">-->
<!--          <iframe src="https://www.youtube.com/embed/WEgJf1WC5SQ"-->
<!--                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Method</h2>
        <div class="content">
          <img src="my_data/overview4.pdf" width="100%"/>
        </div>
        </br>
        <div class="content has-text-justified">
          <p>
            As illustrated in the above figure, we decompose DiffTalker into
            two agent networks: (1) an audio-guided landmark predic-
            tion network and (2) a landmark-guided face generation
            network. The landmark prediction network takes the upper
            half of the landmarks as input and generates the remaining
            key points with the support of audio features. It comprises
            three transformer-based modules, responsible for predicting
            the lower half of the face (LF-Trans), establishing the basic
            mouth shape (BM-Trans), and adjusting the mouth based on
            audio input (AM-Trans). We view the landmark results as
            supplementary information that assists the diffusion model
            in generating precise facial geometry, particularly for mouth
            shape. The diffusion-based face prediction network takes
            both Gaussian noise and the predicted landmarks as input,
            producing the facial texture. Similar to most methods,
            the landmark features interact with image features through
            cross-attention. By employing landmarks, we bridge the do-
            main gap between the audio and image spaces without the
            need for training an additional aligner network.
          </p>
          </br>
        </div>
      </div>
    </div>
    <!--/ Method. -->
  </div>
</section>


<!--<section class="hero is-light is-small">-->
<!--  <div class="hero-body">-->
<!--    <div class="container has-text-centered">-->
<!--      &lt;!&ndash; <h2 class="title">Qualitative Results</h2> &ndash;&gt;-->
<!--      <div id="results-carousel" class="carousel results-carousel">-->
<!--        <div class="item rounded-0 border-0 item-sink">-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/sink1.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/sink2.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/sink3.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item rounded-0 border-0 item-piano">-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/piano1.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/piano2.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/piano3.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item rounded-0 border-0 item-light">-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/light1.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/light2.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/light3.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        &lt;!&ndash; <div class="item rounded-0 border-0 item-stove">-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/stove1.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/stove2.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/stove3.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div> &ndash;&gt;-->
<!--        &lt;!&ndash; <div class="item rounded-0 border-0 item-bucket">-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/bucket1.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/bucket2.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/bucket3.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div> &ndash;&gt;-->
<!--        <div class="item rounded-0 border-0 item-table">-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/table1.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/table2.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/table3.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--       &lt;!&ndash;  <div class="item rounded-0 border-0 item-chess">-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/chess1.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/chess2.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/chess3.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div> &ndash;&gt;-->
<!--        <div class="item rounded-0 border-0 item-chess2">-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/chess1.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/chess4.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/chess5.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        &lt;!&ndash; <div class="item rounded-0 border-0 item-hat">-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/hat1.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/hat2.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop height="100%" preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/hat3.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div> &ndash;&gt;-->
<!--        <div class="item rounded-0 border-0 item-rock">-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/rock1.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/rock2.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/rock3.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item rounded-0 border-0 item-statue">-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/statue1.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/statue2.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/statue3.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        &lt;!&ndash; <div class="item rounded-0 border-0 item-statue2">-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/statue1.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/statue4.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/statue5.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div> &ndash;&gt;-->
<!--        <div class="item rounded-0 border-0 item-stairs">-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/stairs1.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/stairs2.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/stairs3.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        &lt;!&ndash; <div class="item rounded-0 border-0 item-dumbbell">-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/dumbbell1.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/dumbbell2.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/dumbbell3.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div> &ndash;&gt;-->
<!--        <div class="item rounded-0 border-0 item-leaves">-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/leaves1.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/leaves2.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/leaves3.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        &lt;!&ndash; <div class="item rounded-0 border-0 item-fortress">-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/fortress1.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/fortress2.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <video poster="" autoplay muted loop preload="none" class="w-auto" style="height:22vh;">-->
<!--            <source src="data/videos/fortress3.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div> &ndash;&gt;-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->




<!-- <br/>
<section class="hero teaser">
  <div class="container is-max-desktop has-text-centered">
    <br/>
    <h2 class="title">Sample Scene from Our Dataset</h2>
    <div class="row d-flex justify-content-between">
      <div class=col-3>
          <div class="row">
              <div class="content has-text-justified">
                  <img src="my_data/gif/8.gif" width="100%">
                  <h5 class="subtitle has-text-centered">sys #1</h5>
              </div>
          </div>
<br/>
          <div class="row">
              <div class="content has-text-justified">
                 <img src="my_data/gif/11.gif" width="100%">
                 <h5 class="subtitle has-text-centered">sys #4</h5>
              </div>
          </div>
<br/>
          <div class="row">
              <div class="content has-text-justified">
                <img src="my_data/gif/real1.gif" width="100%">
                <h5 class="subtitle has-text-centered">real #1</h5>
              </div>
          </div>
      </div>

      <div class="col-3">
          <div class="row">
              <div class="content has-text-justified">
                <img src="my_data/gif/9.gif" width="100%">
                <h5 class="subtitle has-text-centered">sys #2</h5>
              </div>
          </div>
<br/>
          <div class="row">
              <div class="content has-text-justified" >
                <img src="my_data/gif/12.gif" width="100%">
                <h5 class="subtitle has-text-centered">sys #5</h5>
              </div>
          </div>
<br/>
          <div class="row">
              <div class="content has-text-justified" >
                <img src="my_data/gif/real2.gif" width="100%">
                <h5 class="subtitle has-text-centered">sys #2</h5>
              </div>
          </div>
      </div>


      <div class="col-3">
          <div class="row">
              <div class="content has-text-justified" >
                <img src="my_data/gif/10.gif" width="100%">
                <h5 class="subtitle has-text-centered">sys #3</h5>
              </div>
          </div>
<br/>
          <div class="row">
              <div class="content has-text-justified">
                <img src="my_data/gif/14.gif" width="100%">
                <h5 class="subtitle has-text-centered">sys #6</h5>
              </div>
          </div>
<br/>
          <div class="row">
              <div class="content has-text-justified">
                <img src="my_data/gif/real3.gif" width="100%">
                <h5 class="subtitle has-text-centered">real #3</h5>
              </div>
          </div>
      </div>

    </div>
  </div>
</section>
<br/> -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Results: Landmark completion</h2>
        <div class="content">
          <img src="my_data/landmark_completion.png" width="60%"/>
        </div>
        </br>
        <div class="content has-text-justified">
          <p>
            The above figure presents a comparison between landmark completion
            and ground truth. In the left portion of the results, it is evident
            that the landmark completion network accurately predicts the
            coordinates of the lower key points, which provide geometry
            information for the subsequent face synthesis network. To as-
            sess the impact of audio features on mouth key points, we use
            mismatched landmarks and speech features as inputs, such
            as employing the upper landmark of the 142_th frame and the
            audio features of the 224_th frame. The third column of the
            results demonstrates that the shape of the mouth accurately
            aligns with the facial image, confirming the success of our
            design by solely utilizing the offset of the mouth key points.
          </p>
          </br>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Results: Face synthesis</h2>
        <div class="content">
          <img src="my_data/face_synthesis.png" width="60%"/>
        </div>
        </br>
        <div class="content has-text-justified">
          <p>
            To assess the performance of the generated faces, we utilized
            Five metrics: LSE-D, LSE-C, LD (The landmark distance be-
            tween predictions and GTs), PSNR, and SSIM. From the below table,
            we can find that our method (Ours2) outperforms the GAN-
            based method and is close to the current diffusion model.
            However, the DAE. needs additional training of an aligner for
            audio features and image features. When we remove the land-
            mark information and directly use the DeepSpeech features as
            the controllable information (replacing the text prompt), the
            performance of our model (Ours1) significantly deteriorates.
            This underscores the efficiency of the landmark as an additional
            information. Figure 3 showcases the qualitative outcomes obtained
            from our face synthesis network. The illustration highlights
            that the generated faces exhibit a coherent geometric structure

            The above figure showcases the qualitative outcomes obtained
            from our face synthesis network. The illustration highlights
            that the generated faces exhibit a coherent geometric structure
            across various head poses, owing to the guidance from the
            upper face and complete landmark information. The lower
            part of the figure displays the landmark prediction results,
            reflecting the geometric accuracy of the synthesized face.

          </p>
          </br>
        </div>
        <div class="content">
          <img src="my_data/table1.png" width="60%"/>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Conculution</h2>
        </br>
        <div class="content has-text-justified">
          <p>
            In this paper, we introduce DiffTalker, a co-driven diffusion
            method tailored for generating talking faces. DiffTalker con-
            sists of two sub-agents: a landmark completion network and
            a face synthesis network. By harnessing landmarks, we es-
            tablish a seamless connection between the audio domain and
            images, thereby enhancing the networkâs capability to gener-
            ate precise geometry and mouth shape results. Experimental
            results demonstrate DiffTalkerâs exceptional performance in
            producing clear and geometrically accurate talking faces, all
            without the requirement for additional alignment between au-
            dio and image features.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Disscusion</h2>
        </br>
        <div class="content has-text-justified">
          <p>
            Our method can make full use of landmark information to generate images consistent with speech. Avoid unnecessary training of an equalizer for audio features and image features. 
            However, there is still room for improvement in the continuity and consistency between frames of our method. 
            This has a lot to do with the accuracy of landmark prediction and inter-frame continuity (there is a certain error in using the results of ffmep as ground truth itself). 
            In future work, we will improve the inter-frame continuity of predicted landmarks to further ensure the quality of the videos we generate.
          </p>
          </br>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Video Clips</h2>
        </br>
      </div>
    </div>
  <div style="display: flex; justify-content: space-between;">
    <div>
      <video width="300" controls>
        <source src="my_data/38.mp4" type="video/mp4">
      </video>
      <p>the video clip of the predicted images.</p>
    </div>
    <div>
      <video width="300" controls>
        <source src="my_data/38_landmarks_predict.mp4" type="video/mp4">
      </video>
      <p> the video clip of the predicted landmarks.</p>
    </div>
    <div>
      <video width="300" controls>
        <source src="my_data/38_landmarks.mp4" type="video/mp4">
      </video>
      <p>the video clip of the ground truth.</p>
    </div>
  </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">More Results</h2>
        </br>
        <div class="content">
          <img src="my_data/moreresults.png" width="60%"/>
        </div>
      </div>
    </div>
  </div>
</section>


<!--<section class="hero teaser">-->
<!--  <div class="hero-body">-->
<!--  <div class="container is-max-desktop has-text-centered">-->
<!--    <h2 class="title">Inpainting Stages</h2>-->
<!--      <div class="row d-flex justify-content-between">-->
<!--        <div class=col-3>-->
<!--          <div class="row">-->
<!--            <video poster="" autoplay muted loop height="100%" class="rounded pb-1" preload="none">-->
<!--              <source src="data/videos/interactive_seg1.mp4"-->
<!--                      type="video/mp4">-->
<!--            </video>-->
<!--          </div>-->
<!--          <div class="row">-->
<!--            <video poster="" autoplay muted loop height="100%" class="rounded pb-1" preload="none">-->
<!--              <source src="data/videos/interactive_seg2.mp4"-->
<!--                      type="video/mp4">-->
<!--            </video>-->
<!--          </div>-->
<!--          <div class="row">-->
<!--            <video poster="" autoplay muted loop height="100%" class="rounded pb-1" preload="none">-->
<!--              <source src="data/videos/interactive_seg3.mp4"-->
<!--                      type="video/mp4">-->
<!--            </video>-->
<!--          </div>-->
<!--          <h2 class="subtitle has-text-centered">1) Interactive Segmentation</h2>-->
<!--        </div>-->

<!--        <div class="col-1 align-self-center">-->
<!--          <svg xmlns="http://www.w3.org/2000/svg" width="100%" fill="currentColor" class="bi bi-arrow-right" viewBox="0 0 16 16">-->
<!--            <path fill-rule="evenodd" d="M1 8a.5.5 0 0 1 .5-.5h11.793l-3.147-3.146a.5.5 0 0 1 .708-.708l4 4a.5.5 0 0 1 0 .708l-4 4a.5.5 0 0 1-.708-.708L13.293 8.5H1.5A.5.5 0 0 1 1 8z"/>-->
<!--          </svg>-->
<!--        </div>-->

<!--        <div class="col-3">-->
<!--          <div class="row">-->
<!--            <video poster="" autoplay muted loop height="100%" class="rounded pb-1" preload="none">-->
<!--              <source src="data/videos/table3.mp4" type="video/mp4">-->
<!--            </video>-->
<!--          </div>-->
<!--          <div class="row">-->
<!--            <video poster="" autoplay muted loop height="100%" class="rounded pb-1" preload="none">-->
<!--              <source src="data/videos/statue5.mp4" type="video/mp4">-->
<!--            </video>-->
<!--          </div>-->
<!--          <div class="row">-->
<!--            <video poster="" autoplay muted loop height="100%" class="rounded pb-1" preload="none">-->
<!--              <source src="data/videos/statue3.mp4" type="video/mp4">-->
<!--            </video>-->
<!--          </div>-->
<!--          <h2 class="subtitle has-text-centered">2) Multiview Segmentation</h2>-->
<!--        </div>-->

<!--        <div class="col-1 align-self-center">-->
<!--          <svg xmlns="http://www.w3.org/2000/svg" width="100%" fill="currentColor" class="bi bi-arrow-right" viewBox="0 0 16 16">-->
<!--            <path fill-rule="evenodd" d="M1 8a.5.5 0 0 1 .5-.5h11.793l-3.147-3.146a.5.5 0 0 1 .708-.708l4 4a.5.5 0 0 1 0 .708l-4 4a.5.5 0 0 1-.708-.708L13.293 8.5H1.5A.5.5 0 0 1 1 8z"/>-->
<!--          </svg>-->
<!--        </div>-->

<!--        <div class="col-3">-->
<!--          <div class="row">-->
<!--            <video poster="" autoplay muted loop height="100%" class="rounded pb-1" preload="none">-->
<!--              <source src="data/videos/table2.mp4"-->
<!--                      type="video/mp4">-->
<!--            </video>-->
<!--          </div>-->
<!--          <div class="row">-->
<!--            <video poster="" autoplay muted loop height="100%" class="rounded pb-1" preload="none">-->
<!--              <source src="data/videos/statue4.mp4"-->
<!--                      type="video/mp4">-->
<!--            </video>-->
<!--          </div>-->
<!--          <div class="row">-->
<!--            <video poster="" autoplay muted loop height="100%" class="rounded pb-1" preload="none">-->
<!--              <source src="data/videos/statue2.mp4"-->
<!--                      type="video/mp4">-->
<!--            </video>-->
<!--          </div>-->
<!--          <h2 class="subtitle has-text-centered">3) Multiview Inpainting</h2>-->
<!--        </div>-->
<!--      </div>-->
<!--  </div>-->
<!--  </div>-->
<!--</section>-->

<!--<br/>-->

<!--<section class="hero teaser">-->
<!--  <div class="container is-max-desktop has-text-centered shadow-sm">-->
<!--    <br/>-->
<!--    <h2 class="title">Comparison to the Concurrent Work</h2>-->
<!--      <div class="row">-->
<!--        <div class="col-3">-->
<!--          <video poster="" autoplay muted loop height="100%" class="rounded" preload="none">-->
<!--            <source src="data/videos/hat1.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="col-3">-->
<!--          <video poster="" autoplay muted loop height="100%" class="rounded" preload="none">-->
<!--            <source src="data/videos/hat_nerfin_all.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="col-3">-->
<!--          <video poster="" autoplay muted loop height="100%" class="rounded" preload="none">-->
<!--            <source src="data/videos/hat_nerfin_single.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="col-3">-->
<!--          <video poster="" autoplay muted loop height="100%" class="rounded" preload="none">-->
<!--            <source src="data/videos/hat2.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--      </div>-->
<!--      <div class="row">-->
<!--        <div class="col-3">-->
<!--          <video poster="" autoplay muted loop height="100%" class="rounded" preload="none">-->
<!--            <source src="data/videos/fortress1.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <h5 class="subtitle">NeRF</h5>-->
<!--        </div>-->
<!--        <div class="col-3">-->
<!--          <video poster="" autoplay muted loop height="100%" class="rounded" preload="none">-->
<!--            <source src="data/videos/fortress_nerfin_all.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <h5 class="subtitle">NeRF-In</h5>-->
<!--        </div>-->
<!--        <div class="col-3">-->
<!--          <video poster="" autoplay muted loop height="100%" class="rounded" preload="none">-->
<!--            <source src="data/videos/fortress_nerfin_single.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <h5 class="subtitle">NeRF-In (Single)</h5>-->
<!--        </div>-->
<!--        <div class="col-3">-->
<!--          <video poster="" autoplay muted loop height="100%" class="rounded" preload="none">-->
<!--            <source src="data/videos/fortress2.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--          <h5 class="subtitle">Ours</h5>-->
<!--        </div>-->
<!--      </div>-->
<!--  </div>-->
<!--  </div>-->
<!--</section>-->




<!--<br/>-->


<!--<section class="hero teaser">-->
<!--  <div class="hero-body">-->
<!--    <div class="container is-max-desktop">-->
<!--      <div class="column is-full">-->
<!--        <h2 class="title has-text-centered">On the Importance of the Perceptual Loss</h2>-->
<!--          <div class="row d-flex justify-content-around">-->
<!--            <div class="content has-text-justified">-->
<!--              <p class="content has-text-justified">Here, we demonstrate the importance of using the perceputal loss instead of direct MSE optimization on a 2D toy example. Consider the following RGB image and the synthetic square mask. Based on them, we create the 16 different possible 2D inapintings. Note that inpainting is an ill-posed problem and all of the following are plausible answers for the task of inpainting the image:</p>-->
<!--            </div>-->
<!--            <div class="col-4 p-1">-->
<!--              <img src="data/toy_example/pattern.png">-->
<!--              <h2 class="subtitle has-text-centered">Sample Image</h2>-->
<!--            </div>-->
<!--            <div class="col-4 p-1">-->
<!--              <img src="data/toy_example/masked.png">-->
<!--              <h2 class="subtitle has-text-centered">Masked Image</h2>-->
<!--            </div>-->
<!--            <div class="col-4 p-1">-->
<!--              <img src="data/toy_example/grid.png">-->
<!--              <h2 class="subtitle has-text-centered">16 Different Inpaintings</h2>-->
<!--            </div>-->
<!--            <div class="content has-text-justified">-->
<!--              <p class="content">Now, we start optimizing an image based on these 16 outputs. In the first attempt, the Mean Squared Error (MSE) loss is used, while in the alternative approach, we use a perceptual loss as proposed in the paper for the masked region:</p>-->
<!--            </div>-->
<!--            <div class="row justify-content-around">-->
<!--              <div class="col-8">-->
<!--                <video poster="" muted controls preload="metadata" class="w-auto" style="">-->
<!--                <source src="data/toy_example/toy.mp4"-->
<!--                        type="video/mp4">-->
<!--                </video>-->
<!--              </div>-->
<!--            </div>-->
<!--            <div class="content has-text-justified">-->
<!--              <p class="content">As evident in the results, even after these few steps of fitting the output image on the 16 input inpaintings, the perceptual loss has led to a more detailed and accurate texture. In contrast, the MSE loss has difficulties when facing inconsistent inputs, and has converged to a blurry inpainted region. This blurry area is close to the average of all of the inputs. </p>-->
<!--            </div>-->
<!--          </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{
      @article{qi2023difftalker,
        title={DiffTalker: Co-driven audio-image diffusion for talking faces via intermediate landmarks},
        author={Qi, Zipeng and Zhang, Xulong and Cheng, Ning and Xiao, Jing and Wang, Jianzong},
        journal={arXiv preprint arXiv:2309.07509},
        year={2023}
}
</code></pre>
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a href="https://github.com/nesf3d/nesf3d.github.io">NeSF</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
